# DEPENDENCIES
import logging
import pandas as pd
from .utils import encode_units
from mlxtend.frequent_patterns import apriori, association_rules

# GET LOGGEER 
logger = logging.getLogger(__name__)

# FUNCTIONALITIES
def market_basket_analysis(orders_data: pd.DataFrame, products_data:pd.DataFrame, ) :
    """
    Description

    Arguments:
    ----------
        orders_data { DataFrame } : 

        products_data { DataFrame } :

    Errors:
    -------
        TypeError :

        InsufficientData : 

    Return:
    -------

    """
    # Type Checking 
    if not isinstance(orders_data, pd.DataFrame):
        logger.warning(f"Expected a pandas DataFrame object, got : {type(orders_data)} instead")
        return repr(TypeError(f"Expected a pandas DataFrame object, got : {type(orders_data)} instead"))

    if not isinstance(products_data, pd.DataFrame):
        logger.warning(f"Expected a pandas DataFrame object, got : {type(products_data)} instead")
        return repr(TypeError(f"Expected a pandas DataFrame object, got : {type(products_data)} instead"))

    # Data Validation
    if (len(orders_data) < 2):
        logger.warning(f"InsufficientData : As the input orders_data if of length : {len(orders_data)}, no further processing possible")
        return repr(f"InsufficientData : As the input orders_data if of length : {len(orders_data)}, no further processing possible")

    if (len(products_data) < 2):
        logger.warning(f"InsufficientData : As the input orders_data if of length : {len(products_data)}, no further processing possible")
        return repr(f"InsufficientData : As the input products_data if of length : {len(products_data)}, no further processing possible")

    
    try:
        # Order frequency calculations
        count = orders_data.groupby('product_id')['order_id'].count().reset_index().rename(columns={'order_id': 'frequency'})
    
        # Remove 'eval_set' column
        #products_data = products_data.drop(columns=['eval_set'])
        orders_data.product_id.nunique()

        count = count.sort_values('frequency', ascending=False)[0:100].reset_index(drop=True)
        count = count.merge(products_data, on='product_id', how='left')
        #logger.info(count.head(10))
        # print(count.head(10))

        freq_products = list(count.product_id)
        #logger.info(freq_products[:10])
        # print(freq_products[:10])

        logger.info(len(freq_products))
        # print(len(freq_products))

        orders_data = orders_data[orders_data.product_id.isin(freq_products)]
        logger.info(orders_data.shape)
        # print(orders_data.shape)

        logger.info(orders_data.order_id.nunique())
        # print(orders_data.order_id.nunique())
        orders_data = orders_data.merge(products_data, on='product_id', how='left')
        # logger.info(orders_data.head())
        # print(orders_data.head())

        basket = orders_data.groupby(['order_id', 'product_name'])['reordered'].count().unstack().reset_index().fillna(0).set_index('order_id')
        # logger.info(basket.head())
        # print(basket.head())

        basket = basket.applymap(encode_units)
        # logger.info(basket.head())
        # print(basket.head())

    

        frequent_items = apriori(basket, min_support=0.005, use_colnames=True, low_memory=True)
        # logger.info(frequent_items.head())
        # print(frequent_items.head())

        rules = association_rules(frequent_items, metric="lift", min_threshold=1, num_itemsets=10)
        logger.info(rules.sort_values('lift', ascending=False))
        # print(rules.sort_values('lift', ascending=False))

        # Product Bundling
        high_lift_conf_rules = rules[(rules['lift'] > 2) & (rules['confidence'] > 0.2)]
        high_lift_conf_rules = high_lift_conf_rules.sort_values(by=['lift', 'confidence'], ascending=False)

        product_bundles = high_lift_conf_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]

        return product_bundles

    except Exception as MarketBasketAnalysisError:
        logger.error(f"MarketBasketAnalysisError: While performing market basket analysis, got error: {repr(MarketBasketAnalysisError)}")
        return (f"MarketBasketAnalysisError: While performing market basket analysis, got error: {repr(MarketBasketAnalysisError)}")


# -*- coding: utf-8 -*-
"""Market_basket.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jFm5SiHvc48L3zy17daIyjl5L89u5g_9
"""

# import pandas as pd

# df = pd.read_csv('order_products.csv')

# df

# df1 = pd.read_csv('products.csv')
# df1

# df2 = pd.read_csv('orders.csv')
# df2


# orders_data.product_id.nunique()

# count = orders_data.groupby('product_id')['order_id'].count().reset_index().rename(columns = {'order_id':'frequency'})
# count = count.sort_values('frequency', ascending=False)[0:100].reset_index(drop = True)
# count = count.merge(products_data, on = 'product_id', how = 'left')
# count.head(10)

# freq_products = list(count.product_id)
# freq_products[:10]

# len(freq_products)

# orders_data = orders_data[df.product_id.isin(freq_products)]
# orders_data.shape

# orders_data.order_id.nunique()

# orders_data = orders_data.merge(products_data, on = 'product_id', how='left')
# orders_data.head()

# basket = orders_data.groupby(['order_id', 'product_name'])['reordered'].count().unstack().reset_index().fillna(0).set_index('order_id')
# basket.head()

# def encode_units(x):
#     if x <= 0:
#         return 0
#     if x >= 1:
#         return 1

# basket = basket.applymap(encode_units)
# basket.head()

# from mlxtend.frequent_patterns import apriori
# from mlxtend.frequent_patterns import association_rules

# frequent_items = apriori(basket, min_support=0.005, use_colnames=True, low_memory=True)
# frequent_items.head()

# from mlxtend.frequent_patterns import association_rules

# Assuming frequent_items is your frequent itemsets DataFrame
# rules = association_rules(frequent_items, metric="lift", min_threshold=1, num_itemsets=10)
# rules.sort_values('lift', ascending=False)

# """# **Product Bundling**"""

# high_lift_conf_rules = rules[(rules['lift'] > 2) & (rules['confidence'] > 0.2)]

# high_lift_conf_rules = high_lift_conf_rules.sort_values(by=['lift', 'confidence'], ascending=False)

# high_lift_conf_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]

# rules.sort_values('confidence', ascending=False)?

# rules.to_csv('rules.csv')

# rules

# !ls

